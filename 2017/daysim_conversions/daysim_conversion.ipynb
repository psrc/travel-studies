{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Household, Person, and Trip files to Daysim format for estimation and calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible column names, given that these may change in future surveys\n",
    "hhno = 'hhid'\n",
    "hownrent = 'rent_own'\n",
    "hrestype = 'res_type'\n",
    "hhincome = 'hhincome_detailed'\n",
    "hhtaz = 'final_home_taz2010'\n",
    "hhexpfac = 'hh_wt_revised'\n",
    "hhwkrs = 'numworkers'\n",
    "hhvehs = 'vehicle_count'\n",
    "pno = 'pernum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_persons_to_hh(hh, person, daysim_field, filter_field, \n",
    "                        filter_field_list, hhid_col=hhno, wt_col=hhexpfac):\n",
    "    \n",
    "    \"\"\"Use person field to calculate total number of person in a household for a given field\n",
    "    e.g., total number of full-time workers\"\"\"\n",
    "    \n",
    "    df = person[person[filter_field].isin(filter_field_list)]\n",
    "    df = df.groupby(hhid_col).count().reset_index()[[wt_col,hhid_col]]\n",
    "    df.rename(columns={wt_col: daysim_field}, inplace=True)\n",
    "    \n",
    "    # Join to households\n",
    "    hh = pd.merge(hh, df, how='left', on=hhid_col)\n",
    "    hh[daysim_field].fillna(0, inplace=True)\n",
    "    \n",
    "    return hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lookup maps for various fields\n",
    "hownrent_map = {1:1, # Own: own\n",
    "                2:2, # Rent: rent\n",
    "                3:3, # provided by job/military: other\n",
    "                4:3, # other: other\n",
    "                5:3} # prefer not to answer: other\n",
    "\n",
    "hhrestype_map = {1:1, # SFH: SFH\n",
    "                 2:2, # Townhouse (attached house): duplex/triplex/rowhouse\n",
    "                 3:2, # Building with 3 or fewer apartments/condos: duplex/triplex/rowhouse\n",
    "                 4:3, # Building with 4 or more apartments/condos: apartment/condo\n",
    "                 5:4, # Mobile home/trailer: Mobile home/trailer\n",
    "                 6:5, # Dorm or institutional housing: Dorm room/rented room\n",
    "                 7:6, # other: other\n",
    "                   }\n",
    "\n",
    "# Use the midpoint of the ranges provided since DaySim uses actual values\n",
    "income_map = {\n",
    "    1: 5000,\n",
    "    2: 17500,\n",
    "    3: 30000,\n",
    "    4: 42500,\n",
    "    5: 62500,\n",
    "    6: 87500,\n",
    "    7: 125000,\n",
    "    8: 175000,\n",
    "    9: 225000,\n",
    "    10: 250000,\n",
    "    11: -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hh = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Export\\Version 2\\Restricted\\In-house\\2017-internal-v2-R-1-household.xlsx',\n",
    "                         skiprows=1)\n",
    "person = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Export\\Version 2\\Restricted\\In-house\\2017-internal-v2-R-2-person.xlsx',\n",
    "                      skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\model\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "C:\\Anaconda2\\envs\\model\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Do some up-front data prep\n",
    "# This may be different for new data sets\n",
    "# Identify high school students based on their school name\n",
    "# This will not include all students, but we can start with these students\n",
    "person['high_school'] = 0\n",
    "person['school_loc_name'].fillna(' ', inplace=True)\n",
    "person.ix[(person['school_loc_name'].str.contains(\"High\", na=False)) &\n",
    "          (person['schooltype'].isin([3,4])), \"high_school\"] = 1\n",
    "\n",
    "# Students not in this group will be assumed as high school students\n",
    "# if they're in age group 16-17, and 18-24 and are in K12 (public or private) 3, 4\n",
    "# This is probably excluding some in the 12-15 year group, should try to sort this out better in the future\n",
    "person.ix[(person['high_school'] != 0) & \n",
    "          (person['age'].isin([4,5])) &\n",
    "          (person['schooltype'].isin([3,4])), 'high_school'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Full-time workers\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhftw', filter_field='employment', filter_field_list=[1])\n",
    "\n",
    "# Part-time workers\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhptw', filter_field='employment', filter_field_list=[2])\n",
    "\n",
    "# Retirees\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhret', filter_field='employment', filter_field_list=[6])\n",
    "\n",
    "# Other Adults\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhoad', filter_field='employment', filter_field_list=[3,4,5,7])\n",
    "\n",
    "# University Students\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhuni', filter_field='schooltype', filter_field_list=[6])\n",
    "\n",
    "# High school students\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhhsc', filter_field='high_school', filter_field_list=[1])\n",
    "\n",
    "# k12 age 5-15\n",
    "age5_12 = person[person['age'].isin([2,3])]\n",
    "hh = total_persons_to_hh(hh, age5_12, daysim_field='hh515', filter_field='schooltype', filter_field_list=[3,4])\n",
    "\n",
    "# age under 5\n",
    "hh = total_persons_to_hh(hh, person, daysim_field='hhcu5', filter_field='age', filter_field_list=[1])\n",
    "\n",
    "hh['hownrent'] = hh[hownrent].map(hownrent_map) \n",
    "hh['hrestype'] = hh[hrestype].map(hhrestype_map) \n",
    "hh['hhincome'] = hh[hhincome].map(income_map) \n",
    "hh['hhtaz'] = hh[hhtaz]\n",
    "hh['hhexpfac'] = hh[hhexpfac]\n",
    "hh['hhwkrs'] = hh[hhwkrs]\n",
    "hh['hhno'] = hh[hhno]\n",
    "hh['hhvehs'] = hh[hhvehs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need the parcel ID as well!\n",
    "# Use geopandas to find nearest parcel node?\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# daysim_fields = ['hhno','hhsize','hhvehs','hhwkrs','hhftw','hhptw','hhret','hhoad','hhuni','hhhsc','hh515',\n",
    "#                  'hhcu5','hhincome','hownrent','hrestype','hhparcel','hhtaz','hhexpfac','samptype']\n",
    "# Without parcel field\n",
    "daysim_fields = ['hhno','hhsize','hhvehs','hhwkrs','hhftw','hhptw','hhret','hhoad','hhuni','hhhsc','hh515',\n",
    "                 'hhcu5','hhincome','hownrent','hrestype','hhtaz','hhexpfac']\n",
    "\n",
    "hh[daysim_fields].to_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\household17.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hh[daysim_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload to start with fresh data\n",
    "person = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Export\\Version 2\\Restricted\\In-house\\2017-internal-v2-R-2-person.xlsx',\n",
    "                      skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# person['pernum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:42: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Anaconda2\\envs\\test_python\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "# Reload to start with fresh data\n",
    "person = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Export\\Version 2\\Restricted\\In-house\\2017-internal-v2-R-2-person.xlsx',\n",
    "                      skiprows=1)\n",
    "\n",
    "# Person Type\n",
    "\n",
    "# Full time worker\n",
    "person.ix[person['employment'] == 1, 'pptyp'] = 1\n",
    "\n",
    "# Part-time worker\n",
    "person.ix[person['employment'] == 2, 'pptyp'] = 2\n",
    "\n",
    "# Non-working adult age 65+\n",
    "person.ix[(person['employment'] != 1) &  (person['age'].isin([10,11,12])), 'pptyp'] = 3\n",
    "\n",
    "# High school student age 16+\n",
    "person.ix[(person['age'] >= 4) & (person['schooltype'].isin([3,4,5])), 'pptyp'] = 6\n",
    "\n",
    "# university student (full-time)\n",
    "person.ix[(person['schooltype'].isin([6,7])) & (person['student'] == 3), 'pptyp'] = 5\n",
    "\n",
    "# Child age 5-15\n",
    "person.ix[person['schooltype'].isin([2,3]), 'pptyp'] = 7\n",
    "\n",
    "# child under 5\n",
    "person.ix[person['schooltype'].isin([1]), 'pptyp'] = 8\n",
    "\n",
    "# Non-working adult age 65 should accoutn for all others\n",
    "person.ix[person['pptyp'].isnull(), 'pptyp'] = 4\n",
    "\n",
    "# Person worker type\n",
    "person.ix[person['employment'].isin([1]), 'pwtyp'] = 1\n",
    "person.ix[person['employment'].isin([2]), 'pwtyp'] = 2\n",
    "person.ix[person['employment'].isin([3,4,5,6,7]), 'pwtyp'] = 0\n",
    "person['pwtyp'].fillna(0,inplace=True)\n",
    "person['pwtyp'] = person['pwtyp'].astype('int')\n",
    "\n",
    "# Transit pass availability\n",
    "# Care about people that have subsidized/free passes\n",
    "# people that perceive transit cost as 0\n",
    "person['ptpass'] = 0\n",
    "person.ix[(person['tran_pass_12'].isin([1,2])) | (person['benefits_3'].isin([2,3])),'ptpass'] = 1\n",
    "\n",
    "# Paid parking at work (any subsidization counts as 'paid')\n",
    "person['ppaidprk'] = 0\n",
    "person.ix[person['workpass'].isin([3,4]), 'ppaidprk'] = 1\n",
    "\n",
    "# Take median age\n",
    "age_map = {\n",
    "    1: 2,\n",
    "    2: 8,\n",
    "    3: 14,\n",
    "    4: 17,\n",
    "    5: 21,\n",
    "    6: 30,\n",
    "    7: 40,\n",
    "    8: 50,\n",
    "    9: 60,\n",
    "    10: 70,\n",
    "    11: 80,\n",
    "    12: 85\n",
    "}\n",
    "\n",
    "gender_map = {\n",
    "    1: 1,    # male: male\n",
    "    2: 2,    # female: female\n",
    "    3: 9,    # another: missing\n",
    "    4: 9     # prefer not to answer: missing\n",
    "}\n",
    "\n",
    "pstyp_map = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2\n",
    "}\n",
    "\n",
    "hownrent_map = {\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 3,\n",
    "    4: 3\n",
    "}\n",
    "\n",
    "person['age'] = person['age'].astype('int')\n",
    "person['pagey'] = person['age'].map(age_map)\n",
    "person['pgend'] = person['gender'].map(gender_map)\n",
    "person['pstyp'] = person['student'].map(pstyp_map)\n",
    "person['pstyp'].fillna(0,inplace=True)\n",
    "person['hhno'] = person['hhid']\n",
    "person['pno'] = person['pernum']\n",
    "person['psexpfac'] = person['hh_wt_revised']\n",
    "person['pwtaz'] = -1\n",
    "person['pstaz'] = -1\n",
    "\n",
    "# Need:\n",
    "# pwpcl\n",
    "# pwtaz\n",
    "# pwautime\n",
    "# pwaudist\n",
    "# pspcl\n",
    "# psautime\n",
    "# psaudist\n",
    "# puwmode\n",
    "# puwarrp\n",
    "# puwdepp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone_from_lat_lng(df, taz_gdf, zone_gdf_crs, loc_field, lat_field, lng_field, point_field):\n",
    "    \n",
    "    \"\"\" Joins lat long field to polygon file. \n",
    "        Requires specification of CRS of polyline file\n",
    "        Assumes standard coordinate reference system of 4326 in lat long fields\n",
    "    \"\"\"\n",
    "    lat_lng_crs = 'epsg:4326'\n",
    "    \n",
    "    # Filter for records with valid entries and create gdf\n",
    "    _df = df[-df[loc_field].isnull()]\n",
    "    _df = person[-person[lat_field].isnull()]\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        _df, geometry=gpd.points_from_xy(_df[lng_field], _df[lat_field]))\n",
    "    gdf.crs = {'init' :lat_lng_crs}\n",
    "    \n",
    "    # Align CRS from zone file, convert to lat/long format\n",
    "    taz_gdf.crs = {'init' :'epsg:'+str(zone_gdf_crs)}\n",
    "    taz_gdf = taz_gdf.to_crs({'init': lat_lng_crs})\n",
    "    \n",
    "    # Spatial join between point and polyline\n",
    "    result_df = gpd.sjoin(gdf, taz_gdf, how='left', op='intersects')\n",
    "    \n",
    "    # Update location field\n",
    "    result_df[loc_field] = result_df[point_field]\n",
    "    # Drop locations outside of the region\n",
    "    result_df = result_df[-result_df[loc_field].isnull()]\n",
    "    result_df[loc_field] = result_df[loc_field].astype('int')\n",
    "    \n",
    "    # Join back to original df\n",
    "    df = df.drop(loc_field,axis=1)\n",
    "    df = df.merge(result_df[['hhno','pno',loc_field]], on=['hhno','pno'], how='left')\n",
    "    df[loc_field] = df[loc_field].fillna(-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_gdf = gpd.read_file(r'W:\\geodata\\forecast\\taz2010nowater.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = get_zone_from_lat_lng(person, taz_gdf, zone_gdf_crs='2926', loc_field='pwtaz', \n",
    "                             lat_field='work_lat', lng_field='work_lng', point_field='TAZ')\n",
    "\n",
    "person = get_zone_from_lat_lng(person, taz_gdf, zone_gdf_crs='2926', loc_field='pstaz', \n",
    "                             lat_field='school_loc_lat', lng_field='school_loc_lng', point_field='TAZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daysim_cols = ['hhno', 'pno', 'pptyp', 'pagey', 'pgend', 'pwtyp', 'pwpcl', 'pwtaz', 'pwautime',\n",
    "               'pwaudist', 'pstyp', 'pspcl', 'pstaz', 'psautime', 'psaudist', 'puwmode', 'puwarrp', \n",
    "               'puwdepp', 'ptpass', 'ppaidprk', 'pdiary', 'pproxy', 'psexpfac']\n",
    "\n",
    "# Add empty columns to fill in later with skims\n",
    "for col in daysim_cols:\n",
    "    if col not in person.columns:\n",
    "        person[col] = -1\n",
    "        \n",
    "person = person[daysim_cols]\n",
    "\n",
    "person.to_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\person17.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\model\\lib\\site-packages\\pandas\\io\\excel.py:329: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  **kwds)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'HHID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-4de31d2e887e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m              sheetname='5-Trip-rMove')\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hhno'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HHID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtrip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pno'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PerNum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtrip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DayNum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\model\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\model\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\model\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\model\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\model\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HHID'"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=AWS-PROD-SQL\\Coho;DATABASE=Elmer;Trusted_Connection=yes')\n",
    "# trip = pd.read_sql(sql='select * from HHSurvey.vTrip2017', con=conn)\n",
    "\n",
    "trip = pd.read_excel(r'\\\\aws-prod-file01\\datateam\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Dataset_2 August 2017\\Trips\\5-Trip_rMove-v10-LINKED.xlsx',\n",
    "             sheetname='5-Trip-rMove')\n",
    "\n",
    "trip['hhno'] = trip['HHID']\n",
    "trip['pno'] = trip['PerNum']\n",
    "trip['day'] = trip['DayNum'].astype(int)\n",
    "# Need: \n",
    "# tour\n",
    "# half\n",
    "# tseg\n",
    "trip['tsvid'] = trip['TripNum']\n",
    "\n",
    "# Select only weekday trips (Should we also include Friday?)\n",
    "trip = trip[trip['Dayofweek'].isin(['Monday','Tuesday','Wednesday','Thursday'])]\n",
    "\n",
    "# Recode purposes\n",
    "day_map = {\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4\n",
    "}\n",
    "\n",
    "purpose_map = {\n",
    "    1: 0, # home\n",
    "    6: 2, # school\n",
    "    9: 3, # escort\n",
    "    10: 1, # work\n",
    "    11: 1, # work-related\n",
    "    14: 1, # work-related\n",
    "    30: 5, # grocery -> shop\n",
    "    32: 5, # other shopping -> shop\n",
    "    33: 4,\n",
    "    34: 9, # medical\n",
    "    50: 6, # restaurant -> meal\n",
    "    51: 8, \n",
    "    52: 7,\n",
    "    53: 8,\n",
    "    54: 7, # religious/community/volunteer -> social\n",
    "    56: 7, # family activity -> social\n",
    "    60: 10, # change mode\n",
    "    61: 4,\n",
    "    62: 7, # other social\n",
    "    97: -1 # other\n",
    "}\n",
    "\n",
    "dorp_map = {\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 9\n",
    "}\n",
    "\n",
    "# Survey DB is formatted with string values, need to translate again with above dict\n",
    "df_purp_lookup = pd.read_sql(sql='select * from HHSurvey.DataExplorerValues2017 where VariableID = 125', con=conn)\n",
    "new_purp_map = {}\n",
    "for val in df_purp_lookup['ValueOrder'].unique():\n",
    "    text = df_purp_lookup.loc[df_purp_lookup['ValueOrder'] == val,'ValueText'].values[0]\n",
    "    new_purp_map[text] = purpose_map[val]\n",
    "\n",
    "trip['day'] = trip['Dayofweek'].map(day_map)\n",
    "\n",
    "trip['opurp'] = trip['OriginPurpose'].map(new_purp_map)\n",
    "trip['dpurp'] = trip['DestPurpose'].map(new_purp_map)\n",
    "\n",
    "trip['dorp'] = trip['DestPurpose'].map(new_purp_map)\n",
    "\n",
    "# origin and destination TAZs\n",
    "trip['otaz'] = trip['OTaz2010']\n",
    "trip['dtaz'] = trip['DTaz2010']\n",
    "\n",
    "##############################\n",
    "# Start and end time\n",
    "##############################\n",
    "# Filter out rows with None\n",
    "trip = trip[-trip['DepartTimeTimestamp'].isnull()]\n",
    "trip = trip[-trip['ArrivalTimeTimestamp'].isnull()]\n",
    "\n",
    "# Minutes\n",
    "for db_col_name, daysim_col_name in {'ArrivalTimeTimestamp': 'arrtm', 'DepartTimeTimestamp': 'deptm'}.iteritems():\n",
    "    # Filter rows without valid depart and start times\n",
    "    trip = trip[-trip[db_col_name].isnull()]\n",
    "    \n",
    "    # Get minutes from time stamp, as values to right of :\n",
    "    minutes = trip[db_col_name].apply(lambda row: str(row).split(':')[-1])\n",
    "    minutes = minutes.apply(lambda row: row.split('.')[0]).astype('int') # Trim any decimal places and takes whole numbers\n",
    "    \n",
    "    # Get hours from time stamp\n",
    "    hours = trip[db_col_name].apply(lambda row: str(row).split(' ')[-1].split(':')[0]).astype('int')\n",
    "    \n",
    "    # In minutes after midnight****\n",
    "    trip[daysim_col_name] = hours*60 + minutes\n",
    "    \n",
    "#     # Convert to minutes after 3 AM; sum of minutes minus 3*60 \n",
    "#     trip[daysim_col_name] = (hours*60)+minutes - 180\n",
    "    \n",
    "    # This gives negative values to numbers starting between midnight and 3 am, move these to *after* midnight\n",
    "    # 60*24 (1440) - <negative value>\n",
    "#     trip.loc[trip[daysim_col_name] < 0,daysim_col_name] = 1440 + trip.loc[trip[daysim_col_name] < 0,daysim_col_name]\n",
    "\n",
    "##############################\n",
    "# Mode\n",
    "##############################\n",
    "trip['mode'] = trip['MainMode'].copy()\n",
    "# Get HOV2/HOV3 based on total number of travelers\n",
    "trip.loc[trip['mode'] == 'HOV','mode'] = 'HOV2'\n",
    "trip.loc[(trip['TravelersTotal'] > 2) & (trip['MainMode'] == 'HOV'),'mode'] = 'HOV3+'\n",
    "\n",
    "trip.loc[trip['Mode1'] == 'Other hired service (e.g., Lyft, Uber)','mode'] = 'TNC'\n",
    "\n",
    "# Lookup values\n",
    "mode_dict = {\n",
    "    'Walk': 1,\n",
    "    'Bike': 2,\n",
    "    'SOV': 3,\n",
    "    'HOV2': 4,\n",
    "    'HOV3+': 5,\n",
    "    'Transit': 6,\n",
    "    'TNC': 9,\n",
    "    'Other': 10\n",
    "}\n",
    "\n",
    "trip['mode'] = trip['mode'].map(mode_dict)\n",
    "\n",
    "trip['trexpfac'] = trip['TripWtFinal']\n",
    "\n",
    "trip['travcost'] = -1\n",
    "trip['travtime'] = -1\n",
    "trip['travdist'] = -1\n",
    "\n",
    "trip_cols = ['hhno','pno','day','mode','opurp','dpurp','deptm','otaz','dtaz','arrtm','trexpfac','travcost','travtime','travdist']\n",
    "\n",
    "trip = trip[-trip['mode'].isnull()]\n",
    "trip = trip[-trip['opurp'].isnull()]\n",
    "trip = trip[-trip['dpurp'].isnull()]\n",
    "trip = trip[-trip['otaz'].isnull()]\n",
    "trip = trip[-trip['dtaz'].isnull()]\n",
    "\n",
    "# Write to file\n",
    "trip = trip[trip_cols]\n",
    "trip.to_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\trip17.csv', index=False)\n",
    "\n",
    "# Attach skim values in a separate process \n",
    "# https://github.com/psrc/travel-modeling/blob/master/survey/survey_attach_skims/2017/attach_skim_values_2017_no_tours.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tours\n",
    "Generate a tour file from the trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the trip file with skim values attached\n",
    "trip = pd.read_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\skims_attached\\tripP17_w.dat')\n",
    "\n",
    "### TEMP: Fix ME\n",
    "# Join with the day column - should remove this once skims are attached the file above\n",
    "_trip = pd.read_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\trip17.csv')\n",
    "cols = [u'hhno', u'pno', u'mode', u'opurp', u'dpurp', u'deptm', u'otaz',\n",
    "       u'dtaz', u'arrtm', u'trexpfac']\n",
    "trip = trip.merge(_trip[cols+['day']], on=cols, how='inner')\n",
    "trip = trip.drop_duplicates()\n",
    "\n",
    "# Remove any people that haave Nan in day\n",
    "####\n",
    "\n",
    "# Create a unique person record\n",
    "trip['personid'] = trip['hhno'].astype('str')+trip['pno'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\model\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda2\\envs\\model\\lib\\site-packages\\ipykernel_launcher.py:100: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "C:\\Anaconda2\\envs\\model\\lib\\site-packages\\ipykernel_launcher.py:101: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative person :(171440801\n",
      "negative person :(171440801\n",
      "negative person :(171244111\n",
      "negative person :(171244111\n",
      "negative person :(171431541\n",
      "negative person :(171150922\n",
      "negative person :(171423971\n",
      "negative person :(171091091\n",
      "negative person :(171324671\n",
      "negative person :(171324671\n",
      "negative person :(171373821\n",
      "negative person :(171035871\n",
      "negative person :(171035871\n",
      "negative person :(171035871\n",
      "negative person :(171298461\n",
      "negative person :(171298461\n",
      "negative person :(171504073\n",
      "negative person :(171003031\n",
      "negative person :(171003031\n",
      "negative person :(171504072\n",
      "negative person :(171533672\n",
      "negative person :(171328432\n",
      "negative person :(171328432\n",
      "negative person :(171328432\n",
      "negative person :(171396421\n",
      "negative person :(171372521\n",
      "negative person :(171310881\n",
      "negative person :(171052211\n",
      "negative person :(171085952\n",
      "negative person :(171085952\n",
      "negative person :(171085952\n",
      "negative person :(171085952\n",
      "negative person :(171085952\n",
      "negative person :(171488861\n",
      "negative person :(171120241\n",
      "negative person :(171096152\n",
      "negative person :(171096152\n",
      "negative person :(171096152\n",
      "negative person :(171019492\n",
      "negative person :(171497872\n",
      "negative person :(171351454\n",
      "negative person :(171118842\n",
      "negative person :(171022751\n",
      "negative person :(171085951\n",
      "negative person :(171215391\n",
      "negative person :(171141642\n",
      "negative person :(171141642\n",
      "negative person :(171326741\n",
      "negative person :(171421451\n",
      "negative person :(171421451\n",
      "negative person :(171421451\n",
      "negative person :(171421451\n",
      "negative person :(171421451\n",
      "negative person :(171008342\n",
      "negative person :(171489531\n",
      "negative person :(171489531\n",
      "negative person :(171489531\n",
      "negative person :(171505561\n",
      "negative person :(171505561\n",
      "negative person :(171505561\n",
      "negative person :(171087982\n",
      "negative person :(171370081\n",
      "negative person :(171339641\n",
      "negative person :(171504071\n",
      "negative person :(171096572\n",
      "negative person :(171096572\n",
      "negative person :(171465521\n",
      "negative person :(171135582\n",
      "negative person :(171019491\n",
      "negative person :(171351453\n",
      "negative person :(171290822\n",
      "negative person :(171307433\n",
      "negative person :(171204751\n",
      "negative person :(171204751\n",
      "negative person :(171306112\n",
      "negative person :(171351452\n",
      "negative person :(171351452\n",
      "negative person :(171060282\n",
      "negative person :(171328434\n",
      "negative person :(171328434\n",
      "negative person :(171328433\n",
      "negative person :(171328433\n",
      "negative person :(171352772\n",
      "negative person :(171025021\n",
      "negative person :(171006541\n",
      "negative person :(171009641\n",
      "negative person :(171144971\n",
      "negative person :(171144971\n",
      "negative person :(171028192\n",
      "negative person :(171141771\n",
      "negative person :(171141771\n",
      "negative person :(171175032\n",
      "negative person :(171443652\n",
      "negative person :(171443652\n",
      "negative person :(171443652\n",
      "negative person :(171023981\n",
      "negative person :(171006542\n",
      "negative person :(171028193\n",
      "negative person :(171142022\n",
      "negative person :(171375841\n",
      "negative person :(171375841\n",
      "negative person :(171006543\n",
      "negative person :(171389891\n",
      "negative person :(171036722\n",
      "negative person :(171297822\n",
      "negative person :(171157511\n",
      "negative person :(171533671\n",
      "negative person :(171430311\n",
      "negative person :(171028191\n",
      "negative person :(171519451\n",
      "negative person :(171471131\n",
      "negative person :(171274521\n",
      "negative person :(171089181\n",
      "negative person :(171089181\n",
      "negative person :(171256532\n",
      "negative person :(171256532\n",
      "negative person :(171256532\n",
      "negative person :(171135581\n",
      "negative person :(171144621\n",
      "negative person :(171009421\n",
      "negative person :(171009421\n",
      "negative person :(171214441\n",
      "negative person :(171517381\n",
      "negative person :(171517381\n",
      "negative person :(171367031\n",
      "negative person :(171307432\n",
      "negative person :(171307432\n",
      "negative person :(171058641\n",
      "negative person :(171267052\n",
      "negative person :(171221461\n",
      "negative person :(171063491\n",
      "negative person :(171063491\n",
      "negative person :(171120242\n",
      "negative person :(171286441\n",
      "negative person :(171286441\n",
      "negative person :(171045631\n",
      "negative person :(171045631\n",
      "negative person :(171339151\n",
      "negative person :(171003032\n",
      "negative person :(171134842\n",
      "negative person :(171182322\n",
      "negative person :(171294661\n",
      "negative person :(171294661\n",
      "negative person :(171294661\n",
      "negative person :(171256531\n",
      "negative person :(171373823\n",
      "negative person :(171393202\n",
      "negative person :(171393202\n",
      "negative person :(171241721\n",
      "negative person :(171241721\n",
      "negative person :(171291612\n",
      "negative person :(171019471\n",
      "negative person :(171104812\n",
      "negative person :(171104812\n",
      "negative person :(171298462\n",
      "negative person :(171455931\n",
      "negative person :(171455931\n",
      "negative person :(171255761\n",
      "negative person :(171358741\n",
      "negative person :(171255762\n",
      "negative person :(171455932\n",
      "negative person :(171455932\n",
      "negative person :(171346671\n",
      "negative person :(171346671\n",
      "negative person :(171198131\n",
      "negative person :(171465522\n",
      "negative person :(171461311\n",
      "negative person :(171141641\n",
      "negative person :(171348401\n",
      "negative person :(171348401\n",
      "negative person :(171514032\n",
      "negative person :(171514032\n",
      "negative person :(171423972\n",
      "negative person :(171444733\n",
      "negative person :(171423974\n",
      "negative person :(171375842\n",
      "negative person :(171111662\n",
      "negative person :(171232553\n",
      "negative person :(171281701\n",
      "negative person :(171057851\n",
      "negative person :(171164502\n",
      "negative person :(171491233\n",
      "negative person :(171046602\n",
      "negative person :(171224651\n",
      "negative person :(171224651\n",
      "negative person :(171443651\n",
      "negative person :(171383872\n",
      "negative person :(171204991\n",
      "negative person :(171215731\n",
      "negative person :(171319711\n",
      "negative person :(171096012\n",
      "negative person :(171302584\n",
      "negative person :(171302584\n",
      "negative person :(171302583\n",
      "negative person :(171302583\n",
      "negative person :(171521901\n",
      "negative person :(171329243\n",
      "negative person :(171386321\n",
      "negative person :(171215081\n",
      "negative person :(171104811\n",
      "negative person :(171150161\n",
      "negative person :(171381611\n",
      "negative person :(171519452\n",
      "negative person :(171285732\n",
      "negative person :(171370724\n",
      "negative person :(171243111\n",
      "negative person :(171073191\n",
      "negative person :(171073191\n",
      "negative person :(171256991\n",
      "negative person :(171182321\n",
      "negative person :(171284451\n",
      "negative person :(171284451\n",
      "negative person :(171386324\n",
      "negative person :(171175031\n",
      "negative person :(171119601\n",
      "negative person :(171119601\n",
      "negative person :(171394352\n",
      "negative person :(171394352\n",
      "negative person :(171329301\n",
      "negative person :(171266851\n",
      "negative person :(171326791\n",
      "negative person :(171267624\n",
      "negative person :(171171382\n",
      "negative person :(171429092\n",
      "negative person :(171073192\n",
      "negative person :(171270141\n",
      "negative person :(171386442\n",
      "negative person :(171169181\n",
      "negative person :(171339644\n",
      "negative person :(171506041\n",
      "negative person :(171243161\n",
      "negative person :(171380762\n",
      "negative person :(171327311\n",
      "negative person :(171118611\n",
      "negative person :(171518311\n",
      "negative person :(171481811\n",
      "negative person :(171267622\n",
      "negative person :(171202511\n",
      "negative person :(171098984\n",
      "negative person :(171458271\n",
      "negative person :(171289661\n",
      "negative person :(171366093\n",
      "negative person :(171370722\n",
      "negative person :(171492852\n",
      "negative person :(171096011\n",
      "negative person :(171096011\n",
      "negative person :(171101281\n",
      "negative person :(171373321\n",
      "negative person :(171243423\n",
      "negative person :(171079071\n",
      "negative person :(171202513\n",
      "negative person :(171232552\n",
      "negative person :(171234821\n",
      "negative person :(171451811\n",
      "negative person :(171077422\n",
      "negative person :(171379821\n",
      "negative person :(171316541\n",
      "negative person :(171096013\n",
      "negative person :(171096013\n",
      "negative person :(171283251\n",
      "negative person :(171283254\n",
      "negative person :(171241723\n",
      "negative person :(171018861\n",
      "negative person :(171503101\n",
      "negative person :(171440803\n",
      "negative person :(171244112\n",
      "negative person :(171244112\n",
      "negative person :(171225163\n",
      "negative person :(171225164\n",
      "negative person :(171458552\n",
      "negative person :(171225161\n",
      "negative person :(171458551\n",
      "negative person :(171503831\n",
      "negative person :(171335981\n",
      "negative person :(171288962\n",
      "negative person :(171354273\n",
      "negative person :(171283253\n",
      "negative person :(171201892\n",
      "negative person :(171367802\n",
      "negative person :(171367801\n",
      "negative person :(171079023\n",
      "negative person :(171430392\n"
     ]
    }
   ],
   "source": [
    "tour_dict = {}\n",
    "mylist = []\n",
    "bad_trips = []\n",
    "tour_id = 0\n",
    "\n",
    "for personid in trip['personid'].value_counts().index.values:\n",
    "# for personid in ['171000051','171317451']:\n",
    "\n",
    "    person_df = trip.loc[trip['personid'] == personid]\n",
    "    # Loop through each day\n",
    "    for day in person_df['day'].unique():\n",
    "        df = person_df.loc[person_df['day'] == day]\n",
    "    \n",
    "        # First trip record should be home (?)\n",
    "        if df.groupby('personid').first()['opurp'].values[0] != 0:\n",
    "            bad_trips.append(df['personid'].iloc[0])\n",
    "            continue\n",
    "\n",
    "        # identify home tours first, then check for work and other subtours \n",
    "        home_tours_start = df[df['opurp'] == 0]\n",
    "        home_tours_end = df[df['dpurp'] == 0]\n",
    "\n",
    "        # skip person if they have a different number of tour starts/ends at home\n",
    "        if len(home_tours_start) != len(home_tours_end):\n",
    "            bad_trips.append(df['personid'].iloc[0])\n",
    "            continue\n",
    "\n",
    "        # Loop through each set of home-based tours\n",
    "        for set_index in xrange(len(home_tours_start)):\n",
    "\n",
    "            tour_dict[tour_id] = {}       \n",
    "\n",
    "            # start row for this set\n",
    "            start_row_id = home_tours_start.index[set_index]\n",
    "    #         print start_row\n",
    "            end_row_id = home_tours_end.index[set_index]\n",
    "    #         print '-----'\n",
    "            # iterate between the start row id and the end row id to build the tour\n",
    "\n",
    "            # Select slice of trips that correspond to a trip set\n",
    "            _df = df.loc[start_row_id:end_row_id]\n",
    "\n",
    "            #################################\n",
    "            # Skip this trip set under certain conditions\n",
    "            #################################\n",
    "\n",
    "            if len(_df) == 0:\n",
    "                continue\n",
    "\n",
    "            # Trips with negative purposes\n",
    "            if (_df['opurp'] < 0).any() or (_df['dpurp'] < 0).any():\n",
    "                print 'negative person :(' + str(_df['personid'].iloc[0])\n",
    "                bad_trips.append(df['personid'].iloc[0])\n",
    "                continue\n",
    "\n",
    "            # Trips with same opurp and dpurp that is home\n",
    "            if len(_df[(_df['opurp'] == _df['dpurp']) & (_df['opurp'] == 0)]) > 0:\n",
    "                bad_trips.append(df['personid'].iloc[0])\n",
    "                continue\n",
    "\n",
    "    #         # Trips that have different purposes in sequence\n",
    "    #         if len (df[df.shift(-1)['opurp']!=df['dpurp']]) > 0:\n",
    "    #             bad_trips.append(df['personid'].iloc[0])\n",
    "    #             continue\n",
    "\n",
    "            # First row\n",
    "            tour_dict[tour_id]['tlvorig'] = _df.iloc[0]['deptm']\n",
    "            tour_dict[tour_id]['tardest'] = _df.iloc[0]['arrtm']\n",
    "            tour_dict[tour_id]['totaz'] = _df.iloc[0]['otaz']\n",
    "            # NEED PARCEL DATA ON TRIP RECORDS!!!\n",
    "\n",
    "            # Last row\n",
    "            tour_dict[tour_id]['tlvdest'] = _df.iloc[-1]['deptm']\n",
    "            tour_dict[tour_id]['tarorig'] = _df.iloc[-1]['arrtm']\n",
    "            tour_dict[tour_id]['tdtaz'] = _df.iloc[-1]['otaz']\n",
    "\n",
    "            # Household and person info\n",
    "            tour_dict[tour_id]['hhno'] = _df.iloc[0]['hhno']\n",
    "            tour_dict[tour_id]['pno'] = _df.iloc[0]['pno']\n",
    "            tour_dict[tour_id]['day'] = day\n",
    "\n",
    "            # Identify primary purpose and figure out the tour halves\n",
    "        #   ****ASSUMING primary tour is the activity that takes the longest amount of time\n",
    "\n",
    "             # Determine if this is part of the first half tour or second half tour\n",
    "            # calculate duration, as difference between arrival at a place and start of next trip\n",
    "            _df['duration'] = _df.shift(-1).iloc[:-1]['deptm']-_df.iloc[:-1]['arrtm']\n",
    "\n",
    "            if len(_df) > 3:\n",
    "                mylist.append(_df['personid'].iloc[0])\n",
    "\n",
    "            # For tour groups with only 2 trips, the halves are simply the first and second trips\n",
    "            if len(_df) == 2:\n",
    "                tour_dict[tour_id]['pdpurp'] = _df.iloc[0]['dpurp']\n",
    "                tour_dict[tour_id]['tripsh1'] = 1\n",
    "                tour_dict[tour_id]['tripsh2'] = 1\n",
    "            # For tour groups with > 2 trips, calculate primary purpose and halves\n",
    "            else:\n",
    "                # Assuming that the primary purpose is the purpose for the trip to place with longest duration\n",
    "                primary_purp_index = _df['duration'].argmax()\n",
    "                tour_dict[tour_id]['pdpurp'] = _df.loc[_df['duration'].argmax()]['opurp']\n",
    "\n",
    "                # Get the tour DTAZ as the DTAZ of the primary trip destination\n",
    "                tour_dict[tour_id]['tdtaz'] = _df.loc[primary_purp_index]['dtaz']\n",
    "                # destination parcel\n",
    "\n",
    "                # Get number of trips in the first half tour\n",
    "                tour_dict[tour_id]['tripsh1'] = len(_df.iloc[0:primary_purp_index+1])\n",
    "\n",
    "                # trips in second half tour\n",
    "                tour_dict[tour_id]['tripsh2'] = len(_df.iloc[primary_purp_index:])\n",
    "\n",
    "                # look for subtours\n",
    "                ##### FIX ME: #####\n",
    "                # for now just set subtours as 0 - do not use this for tour estimation\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate number of subtours\n",
    "            # trips that have the same origin/dest pairs before returning home\n",
    "\n",
    "    #         print personid\n",
    "\n",
    "            # Extract main mode type\n",
    "            # use a heirarchy of modes used on the trip\n",
    "            mode_list = _df['mode'].value_counts().index.astype('int').values\n",
    "            mode_heirarchy = [3,4,5,6,9,2,1]\n",
    "            for mode in mode_heirarchy:\n",
    "                if mode in mode_list:\n",
    "                    tour_dict[tour_id]['tmodetp'] = mode\n",
    "                    break\n",
    "\n",
    "\n",
    "            tour_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trip\n",
    "# df = trip.loc[(trip['personid'] == '171027872') & (trip['day']==2)]\n",
    "# [i for i in trip['personid'].unique()]\n",
    "# df\n",
    "\n",
    "# # Identify subtours\n",
    "# # Subtours begin with a trip from work to anywhere but home\n",
    "# df[(df['opurp'] == 1) & (df['dpurp'] != 0)]\n",
    "# # It's a work subtour if any of the subsequent trips return to work without returning home\n",
    "# # loop thorugh the next trips\n",
    "# for index, row in df[(df['opurp'] == 1) & (df['dpurp'] != 0)].iterrows():\n",
    "#     if row['dpurp'] == 0:\n",
    "#         break\n",
    "#     if row['dpurp'] == 1:\n",
    "#         end_trip = row\n",
    "# begin_trip = df[(df['opurp'] == 1) & (df['dpurp'] != 0)].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hhno         17102787\n",
       "pno                 2\n",
       "mode                9\n",
       "opurp               1\n",
       "dpurp               1\n",
       "deptm            1120\n",
       "otaz               47\n",
       "dtaz              313\n",
       "arrtm            1135\n",
       "trexpfac      5.51514\n",
       "id               3107\n",
       "travcost            0\n",
       "travdist         4.41\n",
       "travtime        14.91\n",
       "day                 2\n",
       "personid    171027872\n",
       "Name: 43093, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_dict(tour_dict, orient='index')\n",
    "result_df['personid'] = result_df['hhno'].astype('str')+result_df['pno'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'hhno', u'pno', u'day', u'tour', u'jtindex', u'parent', u'subtrs',\n",
       "       u'pdpurp', u'tlvorig', u'tardest', u'tlvdest', u'tarorig', u'toadtyp',\n",
       "       u'tdadtyp', u'topcl', u'totaz', u'tdpcl', u'tdtaz', u'tmodetp',\n",
       "       u'tpathtp', u'tautotime', u'tautocost', u'tautodist', u'tripsh1',\n",
       "       u'tripsh2', u'phtindx1', u'phtindx2', u'fhtindx1', u'fhtindx2',\n",
       "       u'toexpfac'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read old tour file to get list of columns\n",
    "_tour = pd.read_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2014Estimation\\New_06_20_16\\inputs\\tourP14.dat', delim_whitespace=True)\n",
    "_tour.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'tdtaz', u'tlvdest', u'tripsh1', u'tmodetp', u'totaz', u'tardest',\n",
       "       u'day', u'hhno', u'tlvorig', u'pdpurp', u'pno', u'tarorig', u'tripsh2',\n",
       "       u'personid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to fille these in\n",
    "for col in [u'jtindex', u'parent', u'subtrs','toadtyp', u'tdadtyp', u'topcl',u'tdpcl'u'tpathtp', \n",
    "            u'tautotime', u'tautocost', u'tautodist',u'tripsh1',\n",
    "       u'tripsh2', u'phtindx1', u'phtindx2', u'fhtindx1', u'fhtindx2']:\n",
    "    result_df[col] = 0 \n",
    "result_df['toexpfac'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdtaz</th>\n",
       "      <th>tlvdest</th>\n",
       "      <th>tripsh1</th>\n",
       "      <th>tmodetp</th>\n",
       "      <th>totaz</th>\n",
       "      <th>tardest</th>\n",
       "      <th>day</th>\n",
       "      <th>hhno</th>\n",
       "      <th>tlvorig</th>\n",
       "      <th>pdpurp</th>\n",
       "      <th>...</th>\n",
       "      <th>topcl</th>\n",
       "      <th>tdpcltpathtp</th>\n",
       "      <th>tautotime</th>\n",
       "      <th>tautocost</th>\n",
       "      <th>tautodist</th>\n",
       "      <th>phtindx1</th>\n",
       "      <th>phtindx2</th>\n",
       "      <th>fhtindx1</th>\n",
       "      <th>fhtindx2</th>\n",
       "      <th>toexpfac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1881.0</td>\n",
       "      <td>1134</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>779</td>\n",
       "      <td>4</td>\n",
       "      <td>17131745</td>\n",
       "      <td>765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2392.0</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>751</td>\n",
       "      <td>1</td>\n",
       "      <td>17131745</td>\n",
       "      <td>745</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1586.0</td>\n",
       "      <td>761</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>17144080</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1586.0</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>17144080</td>\n",
       "      <td>90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1586.0</td>\n",
       "      <td>728</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>17144080</td>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tdtaz  tlvdest  tripsh1  tmodetp   totaz  tardest  day      hhno  tlvorig  \\\n",
       "0  1881.0     1134        0        3     4.0      779    4  17131745      765   \n",
       "1  2392.0     1278        0        3     4.0      751    1  17131745      745   \n",
       "2  1586.0      761        0        3  1298.0      128    1  17144080      105   \n",
       "3  1586.0      723        0        3  1298.0      112    2  17144080       90   \n",
       "4  1586.0      728        0        3  1298.0      119    3  17144080       95   \n",
       "\n",
       "   pdpurp    ...     topcl  tdpcltpathtp  tautotime tautocost  tautodist  \\\n",
       "0     1.0    ...         0             0          0         0          0   \n",
       "1     1.0    ...         0             0          0         0          0   \n",
       "2     0.0    ...         0             0          0         0          0   \n",
       "3     1.0    ...         0             0          0         0          0   \n",
       "4     1.0    ...         0             0          0         0          0   \n",
       "\n",
       "   phtindx1  phtindx2  fhtindx1  fhtindx2  toexpfac  \n",
       "0         0         0         0         0         1  \n",
       "1         0         0         0         0         1  \n",
       "2         0         0         0         0         1  \n",
       "3         0         0         0         0         1  \n",
       "4         0         0         0         0         1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join empty columns\n",
    "result_df.to_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\tour17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tour = result_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# pday = person.copy()\n",
    "tour = pd.read_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\tour17.csv')\n",
    "tour['person_id'] = tour['hhno'].astype('str') + tour['pno'].astype('str')\n",
    "trip = pd.read_csv(r'R:\\e2projects_two\\SoundCastDocuments\\2017Estimation\\trip17.csv')\n",
    "trip['person_id'] = trip['hhno'].astype('str') + trip['pno'].astype('str')\n",
    "\n",
    "pday_survey = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Export\\Version 2\\Public\\2017-pr2-4-day.xlsx',\n",
    "                           sheet_name='4-Day-v3', skiprows=1)\n",
    "pday_survey['personid'] = pday_survey['hhid'].astype('str') + pday_survey['pernum'].astype('str')\n",
    "pday_survey['telework_time'] = pday_survey['telework_time'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "# hhno\n",
    "# pno\n",
    "\n",
    "# Work through each person's day using tour file\n",
    "# day\n",
    "person['id'] = person['hhno'].astype('str') + person['pno'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pday = pd.DataFrame()\n",
    "for person_rec in person['id'].unique():\n",
    "    \n",
    "    # get this person's tours\n",
    "    _tour = tour[tour['person_id'] == person_rec]\n",
    "    \n",
    "    # Loop through each day\n",
    "    for day in _tour['day'].unique():\n",
    "        \n",
    "        # from survey data\n",
    "        \n",
    "        _pday_survey = pday_survey[(pday_survey['personid'] == person_rec) & (pday_survey['dayofweek'] == day)]\n",
    "        \n",
    "        day_tour = _tour[_tour['day'] == day]\n",
    "        \n",
    "        prec_id = str(person_rec) + str(day)\n",
    "        pday.loc[prec_id,'hhno'] = day_tour['hhno'].iloc[0]\n",
    "        pday.loc[prec_id,'pno'] = day_tour['pno'].iloc[0]\n",
    "        pday.loc[prec_id,'day'] = day\n",
    "        \n",
    "        # Begin/End at home-\n",
    "        # need to get from first and last trips of tour days \n",
    "        pday.loc[prec_id,'beghom'] = 0\n",
    "        pday.loc[prec_id,'endhom'] = 0\n",
    "        _trip = trip[(trip['person_id'] == person_rec) & (trip['day'] == day)]\n",
    "        if _trip.iloc[0]['opurp'] == 0:\n",
    "            pday.loc[prec_id,'beghom'] = 1\n",
    "        if _trip.iloc[-1]['dpurp'] == 0:\n",
    "            pday.loc[prec_id,'endhom'] = 1\n",
    "            \n",
    "        # Home-based tours\n",
    "        # Work based tours\n",
    "        # Tours to usual workplace in a day\n",
    "    \n",
    "        # Number of tours by purpose\n",
    "        purp_dict = {\n",
    "            'wk': 1,\n",
    "            'sc': 2,\n",
    "            'es': 3,\n",
    "            'pb': 4,\n",
    "            'sh': 5,\n",
    "            'ml': 6,\n",
    "            'so': 7,\n",
    "            're': 8,\n",
    "            'me': 9\n",
    "        }\n",
    "        for purp_name, purp_val in purp_dict.items():\n",
    "            # Number of tours\n",
    "            pday.loc[prec_id,purp_name+'tours'] = len(day_tour[day_tour['pdpurp'] == purp_val])\n",
    "        \n",
    "            # Numbefr of stops\n",
    "            day_tour_purp = day_tour[day_tour['pdpurp'] == purp_val]\n",
    "            pday.loc[prec_id,purp_name+'stops'] = day_tour_purp[['tripsh1','tripsh2']].sum().sum() - 2\n",
    "\n",
    "        pday.loc[prec_id,'wkathome'] = _pday_survey['telework_time'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhno</th>\n",
       "      <th>pno</th>\n",
       "      <th>day</th>\n",
       "      <th>beghom</th>\n",
       "      <th>endhom</th>\n",
       "      <th>wktours</th>\n",
       "      <th>wkstops</th>\n",
       "      <th>sctours</th>\n",
       "      <th>scstops</th>\n",
       "      <th>estours</th>\n",
       "      <th>...</th>\n",
       "      <th>shstops</th>\n",
       "      <th>mltours</th>\n",
       "      <th>mlstops</th>\n",
       "      <th>sotours</th>\n",
       "      <th>sostops</th>\n",
       "      <th>retours</th>\n",
       "      <th>restops</th>\n",
       "      <th>metours</th>\n",
       "      <th>mestops</th>\n",
       "      <th>wkathome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1710000514</td>\n",
       "      <td>17100005.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  hhno  pno  day  beghom  endhom  wktours  wkstops  sctours  \\\n",
       "1710000514  17100005.0  1.0  4.0     1.0     1.0      0.0     -2.0      0.0   \n",
       "\n",
       "            scstops  estours  ...  shstops  mltours  mlstops  sotours  \\\n",
       "1710000514     -2.0      0.0  ...     -2.0      0.0     -2.0      1.0   \n",
       "\n",
       "            sostops  retours  restops  metours  mestops  wkathome  \n",
       "1710000514     -2.0      0.0     -2.0      0.0     -2.0      60.0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    60.0\n",
       "Name: telework_time, dtype: float64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pday_survey['telework_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhno\n",
    "# day\n",
    "# day of week\n",
    "# jttours\n",
    "# ph tours\n",
    "# fh tours\n",
    "# hd exp fac\n",
    "# These don't matter for our person-level model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
